{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d318fc5b",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24963913",
   "metadata": {
    "cell_id": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf \n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import keras_tuner \n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "config = tf.compat.v1.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "def seed_tensorflow(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f405f0",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b669a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = {'data_path':'./input/',\n",
    "        'embedding_dim':64,\n",
    "        'seed':40,\n",
    "        'lr':1e-5,\n",
    "        'batch_size':512,\n",
    "        'epochs':64,\n",
    "        'verbose':1,\n",
    "        'pretrain':False,\n",
    "        'callback':{\n",
    "                     'monitor':'val_loss',\n",
    "                     'patience':5,\n",
    "                     'save_model_path':\"./model/test_fp.h5\"\n",
    "                     },\n",
    "        'mlp_dims':[300,200,100],\n",
    "        'mlp_act':'relu',\n",
    "        'mlp_dps':[.2,.2,.2],\n",
    "        'bias_dim':64,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a483a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = pd.read_csv(para['data_path']+\"action.csv\")\n",
    "feed_emb = np.load(para['data_path']+\"feedid_emb_64.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea482be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = action.sort_values(by=['date_'])\n",
    "\n",
    "train= feed.groupby('userid').apply(lambda x: x[:int(len(x)*0.6)]).reset_index(drop=True)\n",
    "valid= feed.groupby('userid').apply(lambda x: x[int(len(x)*0.6):int(len(x)*0.8)]).reset_index(drop=True)\n",
    "test= feed.groupby('userid').apply(lambda x: x[int(len(x)*0.8):]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223dfd6",
   "metadata": {
    "cell_id": 21
   },
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion(tf.keras.layers.Layer):\n",
    "    name = 'fusion'\n",
    "    def __init__(self,\n",
    "                 alpha = 1.0,\n",
    "                 kl = 0.1,\n",
    "                 fusion_mode='sum',\n",
    "                 activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.kl = kl\n",
    "        self.fusion_mode = fusion_mode\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", \n",
    "            shape=(1,),\n",
    "            initializer=\"one\",\n",
    "        )\n",
    "        super().build(input_shape) \n",
    "        \n",
    "    def call(self, inputs,training=False):\n",
    "        o_base,o_bias = inputs\n",
    "        \n",
    "        base_logit = self._fusion_fuction(o_base,o_bias)\n",
    "        bias_logit = self._fusion_fuction(self.kernel,o_bias) \n",
    "        \n",
    "        base_output = tf.keras.activations.sigmoid(base_logit)\n",
    "        bias_output = tf.keras.activations.sigmoid(bias_logit)\n",
    "        \n",
    "        kl_loss = self.kl * self._kl_loss(base_logit,bias_logit)\n",
    "\n",
    "        if training:\n",
    "            self.add_loss(kl_loss,inputs=True)\n",
    "            return base_output,bias_output\n",
    "        else:\n",
    "            kl_loss = 0.0\n",
    "            self.add_loss(kl_loss,inputs=True)\n",
    "#             base_output = tf.keras.activations.sigmoid(base_logit - self.alpha*bias_logit)\n",
    "            return base_output,bias_output\n",
    "    \n",
    "#     def compute_output_shape(self, batch_input_shape):\n",
    "#         return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         base_config = super().get_config()\n",
    "#         return {**base_config, \"units\": self.units,\n",
    "#                 \"activation\": keras.activations.serialize(self.activation)}   \n",
    "    def _fusion_fuction(self, o1, o2):\n",
    "        eps = 1e-12\n",
    "        if self.fusion_mode == \"sum\":\n",
    "            o_fusion = tf.math.log(tf.sigmoid(o1 + o2)) \n",
    "        if self.fusion_mode == \"hm\":\n",
    "            o = tf.keras.activations.sigmoid(o1) * tf.keras.activations.sigmoid(o2)\n",
    "            o_fusion = tf.math.log(o + eps) - tf.math.log1p(o)\n",
    "        if self.fusion_mode == 'rubi':\n",
    "            o_fusion = o1 * tf.keras.activations.sigmoid(o2)\n",
    "\n",
    "        return o_fusion\n",
    "    \n",
    "    def _kl_loss(self,o1_fusion,o2_fusion):\n",
    "        p_te = tf.nn.sigmoid(o1_fusion)\n",
    "        p_nde = tf.nn.sigmoid(o2_fusion)\n",
    "        kl_loss = -p_te*tf.math.log(p_nde)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        return kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d6dc7",
   "metadata": {
    "cell_id": 3,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "\n",
    "def get_layer(shape,name,dtype='int32',d1=None,d2=None,pretrain=None,trainable=False):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=shape, name=name+'_input', dtype=dtype))\n",
    "    if d1 is None:\n",
    "        d1 = pretrain.shape[0]\n",
    "        d2 = pretrain.shape[1]\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=int(d1),\n",
    "                       output_dim=int(d2),\n",
    "                       weights=[pretrain] if pretrain is not None else None,\n",
    "                       trainable=trainable,\n",
    "                       name=name+'_embedding'))\n",
    "    return model\n",
    "\n",
    "def mlp_bias():\n",
    "    uid_lay = get_layer((1,),'uid',d1 = max(action['userid'])+1,d2 = para['embedding_dim'],trainable=True)\n",
    "    did_lay = get_layer((1,),'did',d1 = max(action['device'])+1,d2 = int(para['embedding_dim']/2),trainable=True)\n",
    "\n",
    "    vid_lay = get_layer((1,),'vid',d1 = max(action['feedid'])+1,d2 = para['embedding_dim'],trainable=True)\n",
    "    aid_lay = get_layer((1,),'aid',d1 = max(action['authorid'])+1,d2 = para['embedding_dim'],trainable=True)\n",
    "    pre_lay = get_layer((1,),'pretrain',pretrain = feed_emb,trainable=False)        \n",
    "    \n",
    "    duration_lay = get_layer((1,),'duration_id',d1 = max(action['duration_level'])+1, d2=para['embedding_dim'],trainable=True)\n",
    "\n",
    "    Lay_bi =[uid_lay,did_lay,vid_lay,aid_lay,pre_lay,duration_lay]\n",
    "    lay_bi_outs = []\n",
    "    for l in Lay_bi:\n",
    "        lay_bi_outs += l.outputs\n",
    "\n",
    "\n",
    "    vec = tf.keras.layers.concatenate(lay_bi_outs,axis=-1)\n",
    "    vec = tf.squeeze(vec,axis=1)\n",
    "    vec = tf.keras.layers.BatchNormalization()(vec)\n",
    "\n",
    "    for i in range(len(para['mlp_dims'])):\n",
    "        vec = tf.keras.layers.Dense(para['mlp_dims'][i],\n",
    "                                    activation = 'relu',\n",
    "                                    name='mlp_dense'+str(i))(vec)\n",
    "        vec = tf.keras.layers.Dropout(para['mlp_dps'][i])(vec)\n",
    "    \n",
    "    model_inputs = []\n",
    "    for l in Lay_bi:\n",
    "        model_inputs += l.inputs\n",
    "        \n",
    "    model = tf.keras.Model(inputs=model_inputs,outputs=[vec,duration_lay.outputs])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def bias(duration_vec):\n",
    "    print(duration_vec.shape)\n",
    "    vec = tf.squeeze(duration_vec,axis=1) # x\n",
    "    vec_dense = tf.keras.layers.Dense(para['bias_dim'],'relu')(vec) # h = f(x)\n",
    "    vec_res = tf.keras.layers.Activation(\"relu\")(tf.keras.layers.Add()([vec,vec_dense])) # g(x+f(x))\n",
    "            \n",
    "    return vec_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4175f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "def crvdd(alpha,fusion_mode,kl):\n",
    "    \n",
    "    backbone = mlp_bias()\n",
    "\n",
    "    vec = backbone.outputs[0]\n",
    "    vec_b = bias(backbone.outputs[1])\n",
    "\n",
    "    logits1 = tf.keras.layers.Dense(1)(vec)\n",
    "    logits2 = tf.keras.layers.Dense(1)(vec_b)\n",
    "\n",
    "    output1,output2 = Fusion(alpha=alpha,fusion_mode=fusion_mode,kl=kl)([logits1,logits2])\n",
    "\n",
    "    model = tf.keras.Model(inputs=backbone.inputs,\n",
    "                           outputs=[output1,output2])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307e2df",
   "metadata": {},
   "source": [
    "# model pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b07d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f6363",
   "metadata": {
    "cell_id": 22
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804925b",
   "metadata": {
    "cell_id": 23
   },
   "outputs": [],
   "source": [
    "features = ['userid','device','feedid','authorid','feedid','duration_level']\n",
    "\n",
    "def get_input(df,is_test=False):\n",
    "    X = []\n",
    "    for f in features:\n",
    "        X.append(df[f].values.reshape(-1,1))\n",
    "    if is_test:\n",
    "        y = [df['test_label'].values.reshape(-1,1)]\n",
    "    else:\n",
    "        y = [df['finish_playing'].values.reshape(-1,1)]\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train = get_input(train,is_test=False)\n",
    "X_valid,y_valid = get_input(valid,is_test=False)\n",
    "X_test,y_test = get_input(test,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe1940d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed_tensorflow(seed=para['seed'])\n",
    "\n",
    "model = crvdd(0,'sum',0.8)\n",
    "o_list = ['fusion', 'fusion_1']\n",
    "adam=tf.keras.optimizers.Adam(learning_rate=para['lr'])\n",
    "\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "                  loss={o_list[0]: 'binary_crossentropy', o_list[1]: 'binary_crossentropy'},\n",
    "                  loss_weights={o_list[0]: 1., o_list[1]: 1.0},\n",
    "                  metrics={o_list[0]: [tf.keras.metrics.AUC(name='auc')],\n",
    "                           o_list[1]:[tf.keras.metrics.AUC(name='auc')]})\n",
    "    \n",
    "# es_callback = tf.keras.callbacks.EarlyStopping(monitor=para['callback']['monitor'],\n",
    "#                                                mode='min',\n",
    "#                                                patience=para['callback']['patience'])\n",
    "\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=para['callback']['save_model_path'],\n",
    "#                                  monitor=para['callback']['monitor'],\n",
    "#                                  mode = 'min',\n",
    "#                                  save_weights_only=True,\n",
    "#                                  save_best_only=True,) \n",
    "\n",
    "# hist = model.fit(X_train,\n",
    "#           {o_list[0]: y_train, o_list[1]: y_train},\n",
    "#           epochs= para['epochs'],\n",
    "#           batch_size=para['batch_size'],\n",
    "#           shuffle=True,\n",
    "#           callbacks = [es_callback,checkpoint],\n",
    "#           verbose=para['verbose'],\n",
    "#           validation_data=(X_valid,(y_valid,y_valid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f311342c",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ee269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topk:\n",
    "    def __init__(self,K,test_df):\n",
    "        self.K = K\n",
    "        self.topk = K\n",
    "        self.test_df = test_df\n",
    "        \n",
    "    def TOPK(self,R,T):\n",
    "        res = {}\n",
    "        p = 4\n",
    "        res['PRECISION@'+str(self.K)] = round(self.PRECISION(R,T),p)\n",
    "        res['RECALL@'+str(self.K)] = round(self.RECALL(R,T),p)\n",
    "        res['HR@'+str(self.K)] = round(self.HR(R,T),p)\n",
    "        res['MAP@'+str(self.K)] = round(self.MAP(R,T),p)\n",
    "        res['MRR@'+str(self.K)] = round(self.MRR(R,T),p)\n",
    "        res['NDCG@'+str(self.K)] = round(self.NDCG(R,T),p)\n",
    "        res['AUTC@'+str(self.K)] = round(self.AUTC(),p)\n",
    "#         print(res)\n",
    "        return res\n",
    "#         return res\n",
    "    \n",
    "    def evaluate(self):\n",
    "        temp = self.test_df[['userid','feedid','play','test_label','pred']]\n",
    "        true_df = temp[temp.test_label == 1].groupby(['userid']).agg({'feedid':lambda x: \\\n",
    "                                           list(x)}).reset_index().sort_values(by=['userid'])  \n",
    "        x=pd.DataFrame(temp[~temp.userid.isin(true_df['userid'])]['userid'].drop_duplicates())\n",
    "        x['feedid'] = [[] for i in range(len(x))]\n",
    "#         true_df = pd.concat([true_df,x])\n",
    "        temp = temp[temp.userid.isin(true_df['userid'])]\n",
    "        temp = temp.sort_values(by=['userid','pred'],ascending=False)\n",
    "        rank_df = temp.groupby(['userid']).agg({'feedid':lambda x: list(x)}).reset_index().sort_values(by=['userid'])\n",
    "        rank_df['top'+str(self.topk)] = rank_df['feedid'].apply(lambda x: x[:self.topk] if len(x)>=self.topk else x)\n",
    "\n",
    "        assert len(true_df) == len(rank_df)\n",
    "\n",
    "        df = pd.merge(left=true_df,right=rank_df[['userid','top'+str(self.topk)]],on=['userid'])\n",
    "\n",
    "        assert len(df) == len(df.dropna(how='any'))\n",
    "\n",
    "        T = df['feedid'].tolist()\n",
    "        R = df['top'+str(self.topk)].tolist()\n",
    "\n",
    "        res = self.TOPK(R,T)\n",
    "        print(\"[%.4f , %.4f ,%.4f,%.4f]\" %(res['RECALL@'+str(self.topk)],\n",
    "                                           res['MAP@'+str(self.topk)],\n",
    "                                           res['NDCG@'+str(self.topk)],\n",
    "                                          res['AUTC@'+str(self.topk)]))\n",
    "#         print(res)\n",
    "#         return res\n",
    "        \n",
    "    def PRECISION(self,R,T):\n",
    "        assert len(R) == len(T)\n",
    "        res = 0\n",
    "        for i in range(len(R)):\n",
    "            res += len(set(R[i])&set(T[i])) / len(R[0])\n",
    "        return res/len(R)\n",
    "\n",
    "    def RECALL(self,R,T):\n",
    "        assert len(R) == len(T)\n",
    "        res = 0\n",
    "        for i in range(len(R)):\n",
    "            if len(T[i]) > 0:\n",
    "                res += len(set(R[i])&set(T[i])) / len(T[i])\n",
    "        return res/len(R)\n",
    "    \n",
    "    def HR(self,R,T):\n",
    "        assert len(R) == len(T)\n",
    "        up = 0\n",
    "        down = len(R)\n",
    "        for i in range(len(R)):\n",
    "            if len(set(R[i])&set(T[i])) > 0:\n",
    "                up += 1\n",
    "        return up / down\n",
    "    \n",
    "    def MAP(self,R,T):\n",
    "        assert len(R) == len(T)\n",
    "        up = 0\n",
    "        down = len(R)\n",
    "        for i in range(len(R)):\n",
    "            temp = 0\n",
    "            hit = 0\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] in T[i]:\n",
    "                    hit += 1\n",
    "                    temp += hit/(j+1)\n",
    "            if hit >0:\n",
    "                up += temp/len(T[i])\n",
    "        return up / down   \n",
    "    \n",
    "    def MRR(self,R,T):\n",
    "        assert len(R) == len(T)\n",
    "        up = 0\n",
    "        down = len(R)\n",
    "        for i in range(len(R)):\n",
    "            index = -1\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] in T[i]:\n",
    "                    index = R[i].index(R[i][j])\n",
    "                    break\n",
    "            if index != -1:\n",
    "                up += 1/(index+1)\n",
    "        return up / down\n",
    "    def dcg(self,hits):\n",
    "        res=0\n",
    "        for i in range(len(hits)):\n",
    "            res += (hits[i]/np.log2(i+2))\n",
    "        return res\n",
    "\n",
    "    def NDCG(self,R,T):\n",
    "        assert len(R) == len(T)\n",
    "        up = 0\n",
    "        down = len(R)\n",
    "        for i in range(len(R)):\n",
    "            hits = []\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] in T[i]:\n",
    "                    hits += [1.0]\n",
    "                else:\n",
    "                    hits += [0.0]\n",
    "            if sum(hits) > 0:\n",
    "                up += (self.dcg(hits) / (self.dcg([1.0 for i in range(len(T[i]))])+1)) #来自wiki的定义，idcg应该是对目标排序。\n",
    "        return up / down \n",
    "    \n",
    "    def AUTC(self):\n",
    "        t = self.test_df.groupby(['userid']).apply(lambda x:self.play_time(x,self.topk))\n",
    "        return t.sum() / len(t)\n",
    "    def play_time(self,x,topk):\n",
    "        temp = x.sort_values(by=['pred'],ascending=False)[:topk]\n",
    "        rank_time = temp['play'].sum()\n",
    "        sum_time = x['play'].sum()\n",
    "        return rank_time / sum_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('./参数/alpha/sum/sum_0.1_wo_kl_0.8_fp.h5')\n",
    "pred = model.predict(X_test)\n",
    "test['base'] = pred[0]\n",
    "test['bias'] = pred[1]\n",
    "test['pred'] = test['base'] - test['bias']\n",
    "test['preds'] = tf.math.sigmoid(test['pred'])\n",
    "\n",
    "Topk(3,test).evaluate()\n",
    "Topk(5,test).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby(['duration_level'])['preds'].mean().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=test.sort_values(by=['userid','pred'],ascending=False)\n",
    "maps = test[['feedid','duration_level']].drop_duplicates(['feedid'])\n",
    "\n",
    "topk=5\n",
    "rank_df = temp.groupby(['userid']).agg({'feedid':lambda x: list(x)}).reset_index().sort_values(by=['userid'])\n",
    "rank_df['top'+str(topk)] = rank_df['feedid'].apply(lambda x: maps[maps.feedid.isin(x[:topk])]['duration_level'].tolist() if len(x)>=topk else x)\n",
    "\n",
    "x=rank_df['top5'].tolist()\n",
    "temp = []\n",
    "for i in x:\n",
    "    temp += i\n",
    "fre = []\n",
    "for i in range(6):\n",
    "    fre += [temp.count(i)]\n",
    "fre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "max_cell_id": 38,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
